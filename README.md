# NPCI_LLM - RAG System

A Retrieval-Augmented Generation (RAG) system built with LlamaIndex for document processing, embedding generation, and intelligent text retrieval.

## ğŸ—ï¸ Architecture

This RAG system is built entirely using **LlamaIndex** for all data operations:

### âœ… **LlamaIndex Integration**
- **Document Loading**: LlamaIndex SimpleDirectoryReader for TXT, PDF, DOCX files
- **Text Chunking**: LlamaIndex SentenceSplitter and TokenTextSplitter
- **Embedding Generation**: LlamaIndex HuggingFace embedding integration
- **Vector Storage**: LlamaIndex Qdrant vector store integration
- **Text Processing**: LlamaIndex Document and TextNode objects

### ğŸ“ **Repository Structure**
```
NPCI_LLM/
â”œâ”€â”€ config/                 # Configuration and settings
â”œâ”€â”€ data/                   # Data ingestion and processing
â”‚   â””â”€â”€ ingestion/         # Document loading, preprocessing, chunking
â”œâ”€â”€ embeddings/            # Embedding generation and management
â”œâ”€â”€ retrieval/             # Vector search and retrieval
â”œâ”€â”€ generation/            # LLM response generation
â”œâ”€â”€ api/                   # FastAPI endpoints
â”œâ”€â”€ tests/                 # Test suites
â”œâ”€â”€ scripts/               # Utility scripts
â”œâ”€â”€ reference_documents/   # Source documents (.txt, .pdf, .docx)
â””â”€â”€ storage/              # Local storage and cache
```

## ğŸš€ **Current Status**

### âœ… **Phase 2: Data Pipeline - COMPLETED**
- Document collection from `reference_documents/`
- Text preprocessing and cleaning
- Intelligent chunking using LlamaIndex
- Integration pipeline validation

### ğŸ”„ **Next Phase: Embeddings & Storage**
- LlamaIndex HuggingFace embedding generation
- Qdrant vector database integration
- Embedding storage and indexing

## ğŸ› ï¸ **Setup**

1. **Install Dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

2. **Environment Configuration**:
   ```bash
   cp env.example .env
   # Update .env with your API keys and settings
   ```

3. **Test Data Pipeline**:
   ```bash
   python test_post_data.py
   ```

## ğŸ“Š **Test Results**

The data pipeline successfully:
- âœ… Loads documents from `reference_documents/`
- âœ… Preprocesses and cleans text content
- âœ… Creates intelligent chunks using LlamaIndex
- âœ… Validates all components work together

## ğŸ”§ **Configuration**

All settings are managed through environment variables in `.env`:
- **Embedding Model**: `sentence-transformers/all-MiniLM-L6-v2` (via LlamaIndex)
- **Vector Database**: Qdrant (localhost:6333)
- **Chunking**: LlamaIndex SentenceSplitter (1024 chars, 200 overlap)
- **LLM**: Mistral (for generation)

## ğŸ“ **Usage**

1. Place your documents in `reference_documents/`
2. Run the data pipeline tests
3. Generate embeddings (Phase 3)
4. Start the API server (Phase 4)

---

**Built with LlamaIndex for robust, production-ready RAG capabilities.**